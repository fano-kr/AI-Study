# %%
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_selection import SelectFromModel, mutual_info_classif
import seaborn as sns
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ
df = sns.load_dataset('titanic')

print("="*60)
print("íƒ€ì´íƒ€ë‹‰ ìƒì¡´ ì˜ˆì¸¡ - ì²´ê³„ì  íŠ¹ì„± ì„ íƒ ì ìš©")
print("="*60)
print(f"ë°ì´í„° í¬ê¸°: {df.shape}")
print(f"ì „ì²´ ì»¬ëŸ¼: {df.columns.tolist()}")

# %%
# =============================================================================
# 1ë‹¨ê³„: ê²°ì¸¡ì¹˜ ë¶„ì„ ë° ê¸°ë³¸ ë°ì´í„° íƒìƒ‰
# =============================================================================
print("\n" + "="*60)
print("1ë‹¨ê³„: ê²°ì¸¡ì¹˜ ë¶„ì„ ë° ê¸°ë³¸ ë°ì´í„° íƒìƒ‰")
print("="*60)

def analyze_missing_values(df):
    """ê²°ì¸¡ì¹˜ ìƒì„¸ ë¶„ì„ í•¨ìˆ˜"""
    missing_info = []
    
    for col in df.columns:
        if col not in ['survived', 'alive']:  # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸
            missing_count = df[col].isnull().sum()
            missing_pct = round(missing_count / len(df) * 100, 2)
            unique_count = df[col].nunique()
            unique_ratio = round(unique_count / len(df), 3)
            
            missing_info.append({
                'Feature': col,
                'Missing_Count': missing_count,
                'Missing_Pct': missing_pct,
                'Unique_Count': unique_count,
                'Unique_Ratio': unique_ratio,
                'Data_Type': str(df[col].dtype)
            })
    
    return pd.DataFrame(missing_info).sort_values('Missing_Pct', ascending=False)

missing_analysis = analyze_missing_values(df)
print("ê²°ì¸¡ì¹˜ ë¶„ì„ ê²°ê³¼:")
print(missing_analysis.to_string(index=False))

# ê²°ì¸¡ì¹˜ê°€ ë§ì€ íŠ¹ì„± í™•ì¸
high_missing_features = missing_analysis[missing_analysis['Missing_Pct'] > 30]['Feature'].tolist()
print(f"\nâš ï¸ ë†’ì€ ê²°ì¸¡ì¹˜ íŠ¹ì„± (>30%): {high_missing_features}")

# %%
# =============================================================================
# 2ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„
# =============================================================================
print("\n" + "="*60)
print("2ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„")
print("="*60)

def prepare_correlation_analysis(df, target_col='survived'):
    """ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬"""
    df_corr = df.copy()
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    categorical_cols = df_corr.select_dtypes(include=['object', 'category']).columns
    label_encoders_temp = {}
    
    for col in categorical_cols:
        if col != target_col:
            le = LabelEncoder()
            df_corr[col] = le.fit_transform(df_corr[col].astype(str))
            label_encoders_temp[col] = le
    
    # boolean íƒ€ì… ë³€í™˜
    bool_cols = df_corr.select_dtypes(include=['bool']).columns
    for col in bool_cols:
        df_corr[col] = df_corr[col].astype(int)
    
    return df_corr, label_encoders_temp

# ìƒê´€ê´€ê³„ ë¶„ì„ìš© ë°ì´í„° ì¤€ë¹„
df_encoded, temp_encoders = prepare_correlation_analysis(df)

# íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
target_corr = df_encoded.corr()['survived'].abs().sort_values(ascending=False)
print("íƒ€ê²Ÿ ë³€ìˆ˜(survived)ì™€ì˜ ìƒê´€ê´€ê³„:")
target_corr_display = target_corr[target_corr.index != 'survived']
for feature, corr in target_corr_display.items():
    print(f"  {feature:<12}: {corr:.3f}")

# ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ìŒ íƒì§€ (ë‹¤ì¤‘ê³µì„ ì„± í™•ì¸)
correlation_matrix = df_encoded.corr()
high_corr_pairs = []

for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_val = abs(correlation_matrix.iloc[i, j])
        if corr_val > 0.8:  # 0.8 ì´ìƒì˜ ë†’ì€ ìƒê´€ê´€ê³„
            high_corr_pairs.append({
                'Feature1': correlation_matrix.columns[i],
                'Feature2': correlation_matrix.columns[j],
                'Correlation': round(corr_val, 3)
            })

print(f"\në†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ìŒ (|r| > 0.8):")
for pair in high_corr_pairs:
    print(f"  {pair['Feature1']} â†” {pair['Feature2']}: {pair['Correlation']}")

# %%
# =============================================================================
# 3ë‹¨ê³„: ìƒí˜¸ì •ë³´ëŸ‰ ë¶„ì„
# =============================================================================
print("\n" + "="*60)
print("3ë‹¨ê³„: ìƒí˜¸ì •ë³´ëŸ‰ ë¶„ì„")
print("="*60)

# ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
X_temp = df_encoded.drop('survived', axis=1)
y_temp = df_encoded['survived']

# ê²°ì¸¡ì¹˜ ì„ì‹œ ì²˜ë¦¬
X_filled = X_temp.copy()
for col in X_filled.columns:
    if X_filled[col].dtype in ['float64', 'int64']:
        X_filled[col].fillna(X_filled[col].mean(), inplace=True)
    else:
        X_filled[col].fillna(X_filled[col].mode()[0] if len(X_filled[col].mode()) > 0 else 0, inplace=True)

# ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°
mi_scores = mutual_info_classif(X_filled, y_temp, random_state=42)
mi_dict = dict(zip(X_temp.columns, mi_scores))

print("ìƒí˜¸ì •ë³´ëŸ‰ ì ìˆ˜ (ë†’ì€ ìˆœ):")
for feature, score in sorted(mi_dict.items(), key=lambda x: x[1], reverse=True):
    print(f"  {feature:<12}: {score:.3f}")

# %%
# =============================================================================
# 4ë‹¨ê³„: íŠ¹ì„± ì„ íƒ ê¸°ì¤€ ì ìš©
# =============================================================================
print("\n" + "="*60)
print("4ë‹¨ê³„: íŠ¹ì„± ì„ íƒ ê¸°ì¤€ ì ìš©")
print("="*60)

def apply_feature_selection_criteria(missing_df, target_corr, mi_dict):
    """ì²´ê³„ì  íŠ¹ì„± ì„ íƒ ê¸°ì¤€ ì ìš©"""
    
    selection_results = []
    
    for _, row in missing_df.iterrows():
        feature = row['Feature']
        issues = []
        
        # ê¸°ì¤€ 1: ê³¼ë„í•œ ê²°ì¸¡ì¹˜ (50% ì´ìƒ)
        if row['Missing_Pct'] > 50:
            issues.append(f"ê³¼ë„í•œ ê²°ì¸¡ì¹˜ ({row['Missing_Pct']:.1f}%)")
        
        # ê¸°ì¤€ 2: ë§¤ìš° ë‚®ì€ ë³€ë™ì„± (unique_ratio < 0.001, ê±°ì˜ ëª¨ë“  ê°’ì´ ë™ì¼í•œ ê²½ìš°ë§Œ)
        if row['Unique_Ratio'] < 0.001:
            issues.append(f"ë§¤ìš° ë‚®ì€ ë³€ë™ì„± ({row['Unique_Ratio']:.3f})")
        
        # ê¸°ì¤€ 3: íƒ€ê²Ÿê³¼ ë§¤ìš° ì•½í•œ ê´€ê³„
        corr_val = target_corr.get(feature, 0)
        mi_val = mi_dict.get(feature, 0)
        if corr_val < 0.05 and mi_val < 0.01:
            issues.append(f"íƒ€ê²Ÿê³¼ ë§¤ìš° ì•½í•œ ê´€ê³„ (corr: {corr_val:.3f}, MI: {mi_val:.3f})")
        
        # ê¸°ì¤€ 4: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì¤‘ë³µì„± í™•ì¸
        redundant_features = {
            'alive': 'survivedì™€ ì™„ì „ ë™ì¼ (íƒ€ê²Ÿ ë³€ìˆ˜)',
            'class': 'pclassì˜ ë²”ì£¼í˜• ë²„ì „ (ì¤‘ë³µ)',
            'embark_town': 'embarkedì˜ ë‹¤ë¥¸ í‘œí˜„ (ì¤‘ë³µ)',
            'adult_male': 'sex + ageë¡œë¶€í„° íŒŒìƒ ê°€ëŠ¥',
            'alone': 'sibsp + parchë¡œë¶€í„° ê³„ì‚° ê°€ëŠ¥ (sibsp==0 and parch==0)',
            'who': 'sex + age ì •ë³´ì˜ ì¡°í•©'
        }
        
        if feature in redundant_features:
            issues.append(f"ì¤‘ë³µì„±: {redundant_features[feature]}")
        
        # ìµœì¢… ê²°ì •
        if len(issues) == 0:
            decision = "âœ… ì„ íƒ"
        elif len(issues) == 1 and any(keyword in issues[0] for keyword in ['ë†’ì€ ê²°ì¸¡ì¹˜', 'ì•½í•œ ê´€ê³„']):
            decision = "âš ï¸ ê³ ë ¤"  # ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì œ
        else:
            decision = "âŒ ì œì™¸"
        
        selection_results.append({
            'Feature': feature,
            'Target_Corr': round(corr_val, 3),
            'Mutual_Info': round(mi_val, 3),
            'Missing_Pct': row['Missing_Pct'],
            'Issues': '; '.join(issues) if issues else 'None',
            'Decision': decision
        })
    
    return pd.DataFrame(selection_results).sort_values('Target_Corr', ascending=False)

# íŠ¹ì„± ì„ íƒ ê¸°ì¤€ ì ìš©
selection_df = apply_feature_selection_criteria(missing_analysis, target_corr, mi_dict)
print("íŠ¹ì„± ì„ íƒ ë¶„ì„ ê²°ê³¼:")
print(selection_df.to_string(index=False))

# %%
# =============================================================================
# 5ë‹¨ê³„: ìµœì¢… íŠ¹ì„± ì„ íƒ ê²°ì •
# =============================================================================
print("\n" + "="*60)
print("5ë‹¨ê³„: ìµœì¢… íŠ¹ì„± ì„ íƒ ê²°ì •")
print("="*60)

# ì„ íƒëœ íŠ¹ì„±ë“¤
selected_features = selection_df[selection_df['Decision'] == 'âœ… ì„ íƒ']['Feature'].tolist()
consider_features = selection_df[selection_df['Decision'] == 'âš ï¸ ê³ ë ¤']['Feature'].tolist()
excluded_features = selection_df[selection_df['Decision'] == 'âŒ ì œì™¸']['Feature'].tolist()

# ê³ ë ¤ íŠ¹ì„± ì¤‘ì—ì„œ ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ì¤‘ìš”í•œ íŠ¹ì„± ì¶”ê°€
important_consider = []
for feature in consider_features:
    # ageëŠ” ê²°ì¸¡ì¹˜ê°€ ìˆì§€ë§Œ ìƒì¡´ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ íŠ¹ì„±
    if feature == 'age':
        important_consider.append(feature)
        print(f"ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ì¶”ê°€: {feature} (ìƒì¡´ ì˜ˆì¸¡ì— ì¤‘ìš”)")

# ìµœì¢… ì„ íƒëœ íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
final_features = selected_features + important_consider

print(f"\nâœ… ìµœì¢… ì„ íƒëœ íŠ¹ì„± ({len(final_features)}ê°œ):")
for i, feature in enumerate(final_features, 1):
    corr_val = selection_df[selection_df['Feature'] == feature]['Target_Corr'].iloc[0]
    mi_val = selection_df[selection_df['Feature'] == feature]['Mutual_Info'].iloc[0]
    missing_pct = selection_df[selection_df['Feature'] == feature]['Missing_Pct'].iloc[0]
    print(f"  {i:2d}. {feature:<12} (ìƒê´€ê´€ê³„: {corr_val:.3f}, MI: {mi_val:.3f}, ê²°ì¸¡ì¹˜: {missing_pct:4.1f}%)")

print(f"\nâŒ ì œì™¸ëœ íŠ¹ì„± ({len(excluded_features)}ê°œ):")
for feature in excluded_features:
    issues = selection_df[selection_df['Feature'] == feature]['Issues'].iloc[0]
    print(f"  - {feature:<12}: {issues}")

# ê¸°ì¡´ ì½”ë“œì™€ì˜ ë¹„êµ
original_features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'class', 'who', 'deck']
print(f"\nğŸ“Š ê¸°ì¡´ vs ê°œì„ ëœ íŠ¹ì„± ì„ íƒ ë¹„êµ:")
print(f"  ê¸°ì¡´ íŠ¹ì„± ({len(original_features)}ê°œ): {original_features}")
print(f"  ê°œì„  íŠ¹ì„± ({len(final_features)}ê°œ): {final_features}")
print(f"  ì œê±°ëœ íŠ¹ì„±: {set(original_features) - set(final_features)}")
print(f"  ì¶”ê°€ëœ íŠ¹ì„±: {set(final_features) - set(original_features)}")

# %%
# =============================================================================
# 6ë‹¨ê³„: ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ëª¨ë¸ êµ¬ì¶•
# =============================================================================
print("\n" + "="*60)
print("6ë‹¨ê³„: ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ëª¨ë¸ êµ¬ì¶•")
print("="*60)

# ìµœì¢… ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„
X = df[final_features].copy()

print(f"ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ëª¨ë¸ êµ¬ì¶•: {final_features}")
print(f"íŠ¹ì„± ë°ì´í„° í¬ê¸°: {X.shape}")

# target ë³€ìˆ˜ ì¸ì½”ë”©
y_encoder = LabelEncoder()
y = y_encoder.fit_transform(df['alive'])  # 'yes'/'no'ë¥¼ 1/0ìœ¼ë¡œ ë³€í™˜

print("íƒ€ê²Ÿ ë³€ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:")
print(f"ì›ë³¸ í´ë˜ìŠ¤: {y_encoder.classes_}")
print(f"ë³€í™˜ëœ ê°’: {y_encoder.transform(y_encoder.classes_)}")

# %%
# =============================================================================
# 7ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ì¸ì½”ë”©)
# =============================================================================
print("\n" + "="*60)
print("7ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬")
print("="*60)

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŠ¹ì„± ë¶„ë¥˜
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

print(f"ìˆ˜ì¹˜í˜• íŠ¹ì„± ({len(numeric_features)}ê°œ): {numeric_features}")
print(f"ë²”ì£¼í˜• íŠ¹ì„± ({len(categorical_features)}ê°œ): {categorical_features}")

# ìˆ˜ì¹˜í˜• ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´)
if numeric_features:
    numeric_imputer = SimpleImputer(strategy='mean')
    X.loc[:, numeric_features] = numeric_imputer.fit_transform(X[numeric_features])
    print(f"ìˆ˜ì¹˜í˜• íŠ¹ì„± ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (í‰ê· ê°’ ëŒ€ì²´)")

# ë²”ì£¼í˜• ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´)
if categorical_features:
    categorical_imputer = SimpleImputer(strategy='most_frequent')
    X.loc[:, categorical_features] = categorical_imputer.fit_transform(X[categorical_features])
    print(f"ë²”ì£¼í˜• íŠ¹ì„± ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (ìµœë¹ˆê°’ ëŒ€ì²´)")

# ë²”ì£¼í˜• ë°ì´í„° ë¼ë²¨ ì¸ì½”ë”©
label_encoders = {}
if categorical_features:
    print("\në²”ì£¼í˜• íŠ¹ì„± ë¼ë²¨ ì¸ì½”ë”©:")
    for feature in categorical_features:
        X[feature] = X[feature].astype(str)
        label_encoders[feature] = LabelEncoder()
        X.loc[:, feature] = label_encoders[feature].fit_transform(X[feature])
        unique_labels = label_encoders[feature].classes_
        print(f"  {feature}: {unique_labels} â†’ {list(range(len(unique_labels)))}")

# íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (StandardScaler ì ìš©)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

print(f"\níŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ (StandardScaler ì ìš©)")
print(f"ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„° í¬ê¸°: {X_scaled.shape}")

# %%
# =============================================================================
# 8ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# =============================================================================
print("\n" + "="*60)
print("8ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
print("="*60)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
print(f"íƒ€ê²Ÿ ë¶„í¬ - í›ˆë ¨: {np.bincount(y_train)}, í…ŒìŠ¤íŠ¸: {np.bincount(y_test)}")

# ê¸°ë³¸ RandomForest ëª¨ë¸ í•™ìŠµ
print("\nğŸŒŸ ê¸°ë³¸ RandomForest ëª¨ë¸ í•™ìŠµ:")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# êµì°¨ ê²€ì¦ ìˆ˜í–‰
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5)
print(f"êµì°¨ ê²€ì¦ ì ìˆ˜: {cv_scores.mean():.3f} (Â±{cv_scores.std() * 2:.3f})")

# í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€
y_pred = rf_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.3f}")

print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
print(classification_report(y_test, y_pred, target_names=y_encoder.classes_))

# %%
# =============================================================================
# 9ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# =============================================================================
print("\n" + "="*60)
print("9ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
print("="*60)

# ê·¸ë¦¬ë“œ ì„œì¹˜ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

print("ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ ì¤‘...")
print(f"íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ì¡°í•© ìˆ˜: {np.prod([len(v) for v in param_grid.values()])}")

# ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    n_jobs=-1,
    scoring='accuracy',
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"\nğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœê³  êµì°¨ ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.3f}")

# %%
# =============================================================================
# 10ë‹¨ê³„: ìµœì í™”ëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
# =============================================================================
print("\n" + "="*60)
print("10ë‹¨ê³„: ìµœì í™”ëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
print("="*60)

# ìµœì í™”ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
best_accuracy = accuracy_score(y_test, y_pred_best)

print(f"ğŸ† ìµœì í™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_accuracy:.3f}")
print(f"ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ: {best_accuracy - test_accuracy:+.3f}")

print("\nìµœì í™”ëœ ëª¨ë¸ ë¶„ë¥˜ ë³´ê³ ì„œ:")
print(classification_report(y_test, y_pred_best, target_names=y_encoder.classes_))

# íŠ¹ì„± ì¤‘ìš”ë„ í™•ì¸
feature_importance = pd.DataFrame({
    'Feature': final_features,
    'Importance': best_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ):")
for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
    print(f"  {i:2d}. {row['Feature']:<12}: {row['Importance']:.3f}")

# %%
# =============================================================================
# 11ë‹¨ê³„: íŠ¹ì„± ì„ íƒ ê¸°ë°˜ ëª¨ë¸ ìµœì í™”
# =============================================================================
print("\n" + "="*60)
print("11ë‹¨ê³„: íŠ¹ì„± ì„ íƒ ê¸°ë°˜ ëª¨ë¸ ìµœì í™”")
print("="*60)

# SelectFromModelì„ ì‚¬ìš©í•œ ì¶”ê°€ íŠ¹ì„± ì„ íƒ
selector = SelectFromModel(best_model, prefit=True)
X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

# ì„ íƒëœ íŠ¹ì„± í™•ì¸
selected_mask = selector.get_support()
selected_feature_names = [final_features[i] for i in range(len(final_features)) if selected_mask[i]]

print(f"ëª¨ë¸ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ ê²°ê³¼:")
print(f"ì›ë³¸ íŠ¹ì„± ìˆ˜: {len(final_features)} â†’ ì„ íƒëœ íŠ¹ì„± ìˆ˜: {len(selected_feature_names)}")
print(f"ì„ íƒëœ íŠ¹ì„±: {selected_feature_names}")

# ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
final_model = RandomForestClassifier(**grid_search.best_params_, random_state=42)
final_model.fit(X_train_selected, y_train)

# ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
y_pred_final = final_model.predict(X_test_selected)
final_accuracy = accuracy_score(y_test, y_pred_final)

print(f"\nğŸŒŸ ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {final_accuracy:.3f}")
print(f"íŠ¹ì„± ì„ íƒ ì „í›„ ì„±ëŠ¥ ë¹„êµ: {final_accuracy:.3f} vs {best_accuracy:.3f} ({final_accuracy - best_accuracy:+.3f})")

# %%
# =============================================================================
# 12ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡
# =============================================================================
print("\n" + "="*60)
print("12ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡")
print("="*60)

# ì›ë³¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìœ„í•œ ì „ì²˜ë¦¬ (í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ì „ì²˜ë¦¬ê¸° ì¬ì‚¬ìš©)
X_full = df[final_features].copy()

print("ì „ì²´ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì¤‘...")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ imputer ì¬ì‚¬ìš©)
if numeric_features:
    X_full.loc[:, numeric_features] = numeric_imputer.transform(X_full[numeric_features])

if categorical_features:
    X_full.loc[:, categorical_features] = categorical_imputer.transform(X_full[categorical_features])

# ë¼ë²¨ ì¸ì½”ë”© (í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ encoder ì¬ì‚¬ìš©)
for feature in categorical_features:
    X_full[feature] = X_full[feature].astype(str)
    X_full.loc[:, feature] = label_encoders[feature].transform(X_full[feature])

# ìŠ¤ì¼€ì¼ë§ (í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ scaler ì¬ì‚¬ìš©)
X_full_scaled = pd.DataFrame(
    scaler.transform(X_full),
    columns=X_full.columns,
    index=X_full.index
)

# íŠ¹ì„± ì„ íƒ ì ìš©
X_full_selected = selector.transform(X_full_scaled)

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
y_pred_full = final_model.predict(X_full_selected)

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
df['predicted_survived'] = y_pred_full
df['predicted_survived_label'] = y_encoder.inverse_transform(y_pred_full)

print(f"ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ì™„ë£Œ: {len(y_pred_full)}ê±´")

# ì‹¤ì œ vs ì˜ˆì¸¡ ë¹„êµ
df_comparison = df.copy()
df_comparison['actual_survived'] = y_encoder.transform(df['alive'])

# ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
full_accuracy = accuracy_score(df_comparison['actual_survived'], df_comparison['predicted_survived'])
print(f"ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ì •í™•ë„: {full_accuracy:.3f}")

# ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ ë¶„ì„
misclassified = df_comparison[df_comparison['actual_survived'] != df_comparison['predicted_survived']]
print(f"ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤: {len(misclassified)}ê±´ ({len(misclassified)/len(df)*100:.1f}%)")

# %%
# =============================================================================
# 13ë‹¨ê³„: ê²°ê³¼ ìš”ì•½ ë° ì‹œê°í™”
# =============================================================================
print("\n" + "="*60)
print("13ë‹¨ê³„: ê²°ê³¼ ìš”ì•½")
print("="*60)

print("ğŸ¯ íƒ€ì´íƒ€ë‹‰ ìƒì¡´ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!")
print("\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:")
print(f"  ğŸ“ˆ ì‚¬ìš©ëœ íŠ¹ì„± ìˆ˜: {len(selected_feature_names)}ê°œ")
print(f"  ğŸ“‹ ì„ íƒëœ íŠ¹ì„±: {selected_feature_names}")
print(f"  ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {final_accuracy:.3f}")
print(f"  ğŸ“Š ì „ì²´ ë°ì´í„° ì •í™•ë„: {full_accuracy:.3f}")

print(f"\nğŸ” íŠ¹ì„± ì„ íƒ íš¨ê³¼:")
original_feature_count = len(['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'class', 'who', 'deck'])
print(f"  ì›ë³¸ íŠ¹ì„± ìˆ˜: {original_feature_count}ê°œ â†’ ìµœì¢… íŠ¹ì„± ìˆ˜: {len(selected_feature_names)}ê°œ")
print(f"  íŠ¹ì„± ê°ì†Œìœ¨: {(original_feature_count - len(selected_feature_names))/original_feature_count*100:.1f}%")

print(f"\nğŸ’¡ ëª¨ë¸ë§ ì¸ì‚¬ì´íŠ¸:")
print(f"  1. ê²°ì¸¡ì¹˜ ë¶„ì„ì„ í†µí•´ deck(77% ê²°ì¸¡ì¹˜) íŠ¹ì„± ì œì™¸")
print(f"  2. ìƒê´€ê´€ê³„ ë¶„ì„ìœ¼ë¡œ ì¤‘ë³µ íŠ¹ì„±(class, embark_town) ì œê±°")
print(f"  3. ë„ë©”ì¸ ì§€ì‹ í™œìš©ìœ¼ë¡œ íŒŒìƒ íŠ¹ì„±(adult_male, alone) ì œì™¸")
print(f"  4. ì²´ê³„ì  íŠ¹ì„± ì„ íƒìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”")

print(f"\nğŸ† ìµœì¢… ëª¨ë¸ íŒŒë¼ë¯¸í„°:")
for param, value in grid_search.best_params_.items():
    print(f"  {param}: {value}")

df.head(10)[['sex', 'age', 'pclass', 'fare', 'alive', 'predicted_survived_label']]

# %%
# =============================================================================
# 14ë‹¨ê³„: ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥
# =============================================================================
print("\n" + "="*60)
print("14ë‹¨ê³„: ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥")
print("="*60)

import joblib
import pickle
import os

# ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
model_dir = 'saved_models'
os.makedirs(model_dir, exist_ok=True)

print("ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥ ì¤‘...")

# 1. ìµœì¢… ëª¨ë¸ ì €ì¥
model_path = os.path.join(model_dir, 'titanic_randomforest_final_model.pkl')
joblib.dump(final_model, model_path)
print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")

# 2. ì „ì²˜ë¦¬ê¸°ë“¤ ì €ì¥
preprocessors = {
    'numeric_imputer': numeric_imputer,
    'categorical_imputer': categorical_imputer,
    'label_encoders': label_encoders,
    'scaler': scaler,
    'feature_selector': selector,
    'target_encoder': y_encoder
}

preprocessor_path = os.path.join(model_dir, 'titanic_preprocessors.pkl')
joblib.dump(preprocessors, preprocessor_path)
print(f"âœ… ì „ì²˜ë¦¬ê¸° ì €ì¥: {preprocessor_path}")

# 3. ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
metadata = {
    'final_features': final_features,
    'selected_features': selected_feature_names,
    'numeric_features': numeric_features,
    'categorical_features': categorical_features,
    'best_params': grid_search.best_params_,
    'test_accuracy': final_accuracy,
    'full_data_accuracy': full_accuracy,
    'model_type': 'RandomForestClassifier',
    'feature_count': len(selected_feature_names),
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
}

metadata_path = os.path.join(model_dir, 'titanic_model_metadata.pkl')
joblib.dump(metadata, metadata_path)
print(f"âœ… ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

# 4. ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥
results_df = df[['sex', 'age', 'pclass', 'fare', 'alive', 'predicted_survived_label']].copy()
results_df.columns = ['ì„±ë³„', 'ë‚˜ì´', 'ë“±ê¸‰', 'ìš”ê¸ˆ', 'ì‹¤ì œ_ìƒì¡´', 'ì˜ˆì¸¡_ìƒì¡´']
results_path = os.path.join(model_dir, 'titanic_predictions.csv')
results_df.to_csv(results_path, index=False, encoding='utf-8-sig')
print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {results_path}")

print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼ ëª©ë¡:")
for file in os.listdir(model_dir):
    file_path = os.path.join(model_dir, file)
    file_size = os.path.getsize(file_path) / 1024  # KB
    print(f"  - {file:<35} ({file_size:.1f} KB)")

# %%
# =============================================================================
# 15ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ í•¨ìˆ˜ ìƒì„±
# =============================================================================
print("\n" + "="*60)
print("15ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ í•¨ìˆ˜")
print("="*60)

def load_titanic_model(model_dir='saved_models'):
    """ì €ì¥ëœ íƒ€ì´íƒ€ë‹‰ ìƒì¡´ ì˜ˆì¸¡ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    
    # ëª¨ë¸ ë¡œë“œ
    model_path = os.path.join(model_dir, 'titanic_randomforest_final_model.pkl')
    model = joblib.load(model_path)
    
    # ì „ì²˜ë¦¬ê¸° ë¡œë“œ
    preprocessor_path = os.path.join(model_dir, 'titanic_preprocessors.pkl')
    preprocessors = joblib.load(preprocessor_path)
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_path = os.path.join(model_dir, 'titanic_model_metadata.pkl')
    metadata = joblib.load(metadata_path)
    
    return model, preprocessors, metadata

def predict_survival(passenger_data, model_dir='saved_models'):
    """
    ìƒˆë¡œìš´ ìŠ¹ê° ë°ì´í„°ì— ëŒ€í•´ ìƒì¡´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        passenger_data (dict): ìŠ¹ê° ì •ë³´ ë”•ì…”ë„ˆë¦¬
        model_dir (str): ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    
    # ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë“œ
    model, preprocessors, metadata = load_titanic_model(model_dir)
    
    # ì…ë ¥ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    input_df = pd.DataFrame([passenger_data])
    
    # í•„ìš”í•œ íŠ¹ì„±ë§Œ ì„ íƒ
    final_features = metadata['final_features']
    input_df = input_df[final_features]
    
    # ì „ì²˜ë¦¬ ì ìš©
    numeric_features = metadata['numeric_features']
    categorical_features = metadata['categorical_features']
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    if numeric_features:
        input_df.loc[:, numeric_features] = preprocessors['numeric_imputer'].transform(input_df[numeric_features])
    
    if categorical_features:
        input_df.loc[:, categorical_features] = preprocessors['categorical_imputer'].transform(input_df[categorical_features])
    
    # ë¼ë²¨ ì¸ì½”ë”©
    for feature in categorical_features:
        input_df[feature] = input_df[feature].astype(str)
        input_df.loc[:, feature] = preprocessors['label_encoders'][feature].transform(input_df[feature])
    
    # ìŠ¤ì¼€ì¼ë§
    input_scaled = preprocessors['scaler'].transform(input_df)
    
    # íŠ¹ì„± ì„ íƒ
    input_selected = preprocessors['feature_selector'].transform(input_scaled)
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    prediction = model.predict(input_selected)[0]
    prediction_proba = model.predict_proba(input_selected)[0]
    
    # ê²°ê³¼ ë³€í™˜
    survival_label = preprocessors['target_encoder'].inverse_transform([prediction])[0]
    
    return {
        'survival_prediction': survival_label,
        'survival_probability': {
            'no': prediction_proba[0],
            'yes': prediction_proba[1]
        },
        'confidence': max(prediction_proba),
        'selected_features': metadata['selected_features'],
        'model_accuracy': metadata['test_accuracy']
    }

# ì˜ˆì¸¡ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
print("ğŸ”® ì˜ˆì¸¡ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:")

# í…ŒìŠ¤íŠ¸ ìŠ¹ê° ë°ì´í„° (íƒ€ì´íƒ€ë‹‰ ì˜í™”ì˜ ì­ê³¼ ë¡œì¦ˆë¥¼ ëª¨ë¸ë¡œ)
test_passengers = [
    {
        'pclass': 3,      # 3ë“±ì„
        'sex': 'male',    # ë‚¨ì„±
        'age': 20,        # 20ì„¸
        'sibsp': 0,       # í˜•ì œìë§¤/ë°°ìš°ì ì—†ìŒ
        'parch': 0,       # ë¶€ëª¨/ìë…€ ì—†ìŒ
        'fare': 7.25,     # ì €ë ´í•œ ìš”ê¸ˆ
        'embarked': 'S'   # Southampton ìŠ¹ì„ 
    },
    {
        'pclass': 1,      # 1ë“±ì„
        'sex': 'female',  # ì—¬ì„±
        'age': 17,        # 17ì„¸
        'sibsp': 1,       # ì•½í˜¼ì ìˆìŒ
        'parch': 2,       # ë¶€ëª¨ 2ëª…
        'fare': 100.0,    # ë¹„ì‹¼ ìš”ê¸ˆ
        'embarked': 'S'   # Southampton ìŠ¹ì„ 
    }
]

test_names = ['ì­ (Jack)', 'ë¡œì¦ˆ (Rose)']

for i, (name, passenger) in enumerate(zip(test_names, test_passengers)):
    try:
        result = predict_survival(passenger, model_dir)
        print(f"\n{i+1}. {name}:")
        print(f"   ì˜ˆì¸¡ ê²°ê³¼: {result['survival_prediction']}")
        print(f"   ìƒì¡´ í™•ë¥ : {result['survival_probability']['yes']:.3f}")
        print(f"   ì‹ ë¢°ë„: {result['confidence']:.3f}")
    except Exception as e:
        print(f"   âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
print(f"   1. ëª¨ë¸ ë¡œë“œ: model, preprocessors, metadata = load_titanic_model()")
print(f"   2. ì˜ˆì¸¡ ìˆ˜í–‰: result = predict_survival(passenger_data)")
print(f"   3. ê²°ê³¼ í™•ì¸: result['survival_prediction'], result['survival_probability']")

print(f"\nğŸ¯ ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì™„ë£Œ!")
print(f"   ì €ì¥ ìœ„ì¹˜: {os.path.abspath(model_dir)}")
print(f"   ëª¨ë¸ ì •í™•ë„: {metadata['test_accuracy']:.3f}")
print(f"   ì‚¬ìš©ëœ íŠ¹ì„±: {len(metadata['selected_features'])}ê°œ")


