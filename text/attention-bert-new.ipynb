{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers\n",
        "%pip install tensorflow\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# 필수 라이브러리 임포트\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 코드 실행 시작 시간 기록\n",
        "start_time = time.time()\n",
        "start_datetime = datetime.now()\n",
        "print(f\"코드 실행 시작: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# 재현 가능한 결과를 위한 시드 설정\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# %%\n",
        "print(transformers.__version__)\n",
        "print(tf.__version__)\n",
        "\n",
        "# %%\n",
        "# pandas read_excel 함수 사용을 위한 openpyxl 설치\n",
        "\n",
        "# %%\n",
        "# 하이퍼파라미터 및 설정 값들을 변수로 정의\n",
        "TRAIN_DATA_URL = 'https://github.com/gzone2000/TEMP_TEST/raw/master/A_comment_train.xlsx'\n",
        "TEST_DATA_URL = 'https://github.com/gzone2000/TEMP_TEST/raw/master/A_comment_test.xlsx'\n",
        "BERT_MODEL_NAME = 'klue/bert-base'\n",
        "MAX_LENGTH = 128  # BERT 입력 시퀀스 최대 길이\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 3e-6  # 학습률을 약간 높여서 더 안정적 학습\n",
        "TEST_SIZE = 0.3\n",
        "\n",
        "# 과적합 방지를 위한 추가 설정\n",
        "# 모델 학습 시 정확도가 높아지지 않아서 추가\n",
        "# 미 설정 시 훈련도가 빠르게 100%에 도달하고 검증 정확도가 96%에서 정체\n",
        "DROPOUT_RATE = 0.1  # 드롭아웃 추가\n",
        "WEIGHT_DECAY = 0.01  # 가중치 감쇠 추가\n",
        "\n",
        "# 데이터 로드\n",
        "try:\n",
        "    comment_train = pd.read_excel(TRAIN_DATA_URL, engine='openpyxl')\n",
        "    comment_test = pd.read_excel(TEST_DATA_URL, engine='openpyxl')\n",
        "    print(f\"훈련 데이터 크기: {comment_train.shape}\")\n",
        "    print(f\"테스트 데이터 크기: {comment_test.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"데이터 로드 중 오류 발생: {e}\")\n",
        "    raise\n",
        "\n",
        "# %%\n",
        "comment_train.head()\n",
        "\n",
        "# %%\n",
        "comment_test.count()\n",
        "\n",
        "# %%\n",
        "# 데이터 전처리 및 정리\n",
        "def preprocess_data(df):\n",
        "    \"\"\"데이터 전처리 함수\"\"\"\n",
        "    # 불필요한 컬럼 제거\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "    \n",
        "    # 결측값 확인 및 처리\n",
        "    null_count = df.isnull().sum().sum()\n",
        "    if null_count > 0:\n",
        "        print(f\"결측값 {null_count}개 발견되었습니다:\")\n",
        "        print(df.isnull().sum())\n",
        "        df = df.dropna()  # 결측값이 있는 행 제거\n",
        "    \n",
        "    # 텍스트 데이터 기본 정리 (공백 제거 등)\n",
        "    df['data'] = df['data'].str.strip()\n",
        "    \n",
        "    # 빈 텍스트 제거\n",
        "    before_len = len(df)\n",
        "    df = df[df['data'].str.len() > 0]\n",
        "    after_len = len(df)\n",
        "    if before_len != after_len:\n",
        "        print(f\"빈 텍스트 {before_len - after_len}개가 제거되었습니다.\")\n",
        "    \n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# 훈련 데이터만 사용 (실제로는 train/validation split을 할 예정)\n",
        "comment = preprocess_data(comment_train.copy())\n",
        "print(f\"전처리 후 데이터 크기: {comment.shape}\")\n",
        "print(f\"레이블 분포:\\n{comment['label'].value_counts()}\")\n",
        "\n",
        "# %%\n",
        "comment.head()\n",
        "\n",
        "# %%\n",
        "comment.isnull().sum()\n",
        "\n",
        "# %%\n",
        "# label 변환\n",
        "#comment['label'] = comment['label'].replace(['부정', '긍정'],[0,1])\n",
        "comment['label'] = comment['label'].apply(lambda x: 0 if x == '부정' else 1)\n",
        "\n",
        "# %%\n",
        "comment.tail()\n",
        "\n",
        "# %%\n",
        "comment.info()\n",
        "\n",
        "# %%\n",
        "# 데이터를 feature와 label로 분리\n",
        "X = comment['data'].tolist()\n",
        "y = comment['label'].tolist()\n",
        "\n",
        "print(f\"총 샘플 수: {len(X)}\")\n",
        "print(f\"긍정 샘플: {sum(y)}개, 부정 샘플: {len(y) - sum(y)}개\")\n",
        "\n",
        "# %%\n",
        "# 훈련/검증 데이터 분할 (stratify를 사용하여 클래스 비율 유지)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    stratify=y, \n",
        "    test_size=TEST_SIZE, \n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"훈련 데이터: {len(X_train)}개\")\n",
        "print(f\"검증 데이터: {len(X_test)}개\")\n",
        "print(f\"훈련 데이터 클래스 분포: 긍정 {sum(y_train)}개, 부정 {len(y_train) - sum(y_train)}개\")\n",
        "print(f\"검증 데이터 클래스 분포: 긍정 {sum(y_test)}개, 부정 {len(y_test) - sum(y_test)}개\")\n",
        "\n",
        "# %%\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)\n",
        "\n",
        "# %%\n",
        "X_train[:2]\n",
        "\n",
        "# %%\n",
        "# BERT 모델 및 토크나이저 초기화\n",
        "print(f\"사용할 BERT 모델: {BERT_MODEL_NAME}\")\n",
        "print(f\"최대 시퀀스 길이: {MAX_LENGTH}\")\n",
        "\n",
        "# %%\n",
        "# BERT 토크나이저와 모델 컴포넌트 임포트 및 초기화\n",
        "from transformers import AutoConfig, BertTokenizerFast, TFBertForSequenceClassification\n",
        "\n",
        "try:\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)\n",
        "    print(f\"토크나이저 로드 완료. 어휘 크기: {tokenizer.vocab_size}\")\n",
        "except Exception as e:\n",
        "    print(f\"토크나이저 로드 중 오류 발생: {e}\")\n",
        "    raise\n",
        "\n",
        "# %%\n",
        "tokenizer.vocab_size\n",
        "\n",
        "# %%\n",
        "#tokenizer.vocab\n",
        "\n",
        "# %%\n",
        "# 텍스트를 BERT 입력 형태로 토크나이징\n",
        "print(\"텍스트 토크나이징 중...\")\n",
        "train_encodings = tokenizer(\n",
        "    X_train, \n",
        "    truncation=True, \n",
        "    padding=True, \n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "test_encodings = tokenizer(\n",
        "    X_test, \n",
        "    truncation=True, \n",
        "    padding=True, \n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "print(f\"훈련 데이터 토크나이징 완료. 형태: {train_encodings['input_ids'].shape}\")\n",
        "print(f\"검증 데이터 토크나이징 완료. 형태: {test_encodings['input_ids'].shape}\")\n",
        "\n",
        "# %%\n",
        "print(train_encodings['input_ids'][0])\n",
        "\n",
        "# %%\n",
        "print(test_encodings['input_ids'][0])\n",
        "\n",
        "# %%\n",
        "# TensorFlow 데이터셋 생성 및 최적화\n",
        "print(\"TensorFlow 데이터셋 생성 중...\")\n",
        "\n",
        "# 훈련 데이터셋 생성 (셔플, 배치, 캐싱, 프리페치 적용)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
        "train_dataset = train_dataset.shuffle(1000, seed=RANDOM_SEED).batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# 검증 데이터셋 생성 (배치, 캐싱, 프리페치 적용)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(f\"데이터셋 생성 완료. 배치 크기: {BATCH_SIZE}\")\n",
        "\n",
        "# %%\n",
        "# BERT 모델 설정 확인\n",
        "config = AutoConfig.from_pretrained(BERT_MODEL_NAME)\n",
        "print(f\"BERT 모델 설정:\")\n",
        "print(f\"- 숨겨진 크기: {config.hidden_size}\")\n",
        "print(f\"- 어텐션 헤드 수: {config.num_attention_heads}\")\n",
        "print(f\"- 숨겨진 레이어 수: {config.num_hidden_layers}\")\n",
        "print(f\"- 어휘 크기: {config.vocab_size}\")\n",
        "print(f\"- 최대 위치 임베딩: {config.max_position_embeddings}\")\n",
        "config\n",
        "\n",
        "# %%\n",
        "# BERT 분류 모델 생성 및 컴파일\n",
        "print(\"BERT 분류 모델 초기화 중...\")\n",
        "\n",
        "# 사전 훈련된 BERT 모델을 분류 작업에 맞게 초기화 (2개 클래스: 긍정/부정)\n",
        "try:\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        BERT_MODEL_NAME, \n",
        "        num_labels=2,  # 긍정/부정 2개 클래스\n",
        "        hidden_dropout_prob=DROPOUT_RATE,  # 드롭아웃 적용\n",
        "        attention_probs_dropout_prob=DROPOUT_RATE,  # 어텐션 드롭아웃\n",
        "        from_pt=True   # PyTorch 모델에서 TensorFlow로 변환\n",
        "    )\n",
        "    print(\"BERT 모델 로드 완료\")\n",
        "except Exception as e:\n",
        "    print(f\"모델 로드 중 오류 발생: {e}\")\n",
        "    raise\n",
        "\n",
        "# 옵티마이저, 손실 함수, 메트릭 설정 (가중치 감쇠 추가)\n",
        "optimizer = tf.keras.optimizers.AdamW(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY  # 가중치 감쇠로 과적합 방지\n",
        ")\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(f\"모델 컴파일 완료:\")\n",
        "print(f\"- 학습률: {LEARNING_RATE}\")\n",
        "print(f\"- 가중치 감쇠: {WEIGHT_DECAY}\")\n",
        "print(f\"- 드롭아웃 비율: {DROPOUT_RATE}\")\n",
        "print(f\"- 손실 함수: SparseCategoricalCrossentropy\")\n",
        "print(f\"- 메트릭: {metrics}\")\n",
        "\n",
        "# %%\n",
        "# 모델 훈련\n",
        "print(\"모델 훈련 시작...\")\n",
        "print(f\"훈련 에포크: {EPOCHS}\")\n",
        "print(f\"배치 크기: {BATCH_SIZE}\")\n",
        "\n",
        "# 콜백 설정 (조기 종료, 체크포인트 저장 등)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# 모델 훈련 실행\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"모델 훈련 완료!\")\n",
        "\n",
        "# %%\n",
        "# 훈련 과정 시각화\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# 손실 그래프\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='훈련 손실')\n",
        "plt.plot(history.history['val_loss'], label='검증 손실')\n",
        "plt.title('모델 손실')\n",
        "plt.xlabel('에포크')\n",
        "plt.ylabel('손실')\n",
        "plt.legend()\n",
        "\n",
        "# 정확도 그래프\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='훈련 정확도')\n",
        "plt.plot(history.history['val_accuracy'], label='검증 정확도')\n",
        "plt.title('모델 정확도')\n",
        "plt.xlabel('에포크')\n",
        "plt.ylabel('정확도')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 최종 성능 출력\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"\\n최종 훈련 결과:\")\n",
        "print(f\"- 훈련 정확도: {final_train_acc:.4f}\")\n",
        "print(f\"- 검증 정확도: {final_val_acc:.4f}\")\n",
        "print(f\"- 훈련 손실: {final_train_loss:.4f}\")\n",
        "print(f\"- 검증 손실: {final_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# %%\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# 실제 테스트 데이터로 모델 성능 평가\n",
        "print(\"실제 테스트 데이터로 모델 평가 중...\")\n",
        "\n",
        "# %%\n",
        "# 레이블이 있는 테스트 데이터 사용 (전처리 적용)\n",
        "# comment_valid = preprocess_data(comment_test.copy())\n",
        "comment_valid = comment_test.copy()\n",
        "print(f\"평가용 테스트 데이터 크기: {comment_valid.shape}\")\n",
        "print(f\"테스트 데이터 레이블 분포:\\n{comment_valid['label'].value_counts()}\")\n",
        "comment_valid.head()\n",
        "\n",
        "# %%\n",
        "comment_valid['data']\n",
        "\n",
        "# %%\n",
        "# 테스트 데이터 예측을 위한 전처리\n",
        "print(\"테스트 데이터 토크나이징 및 예측 중...\")\n",
        "\n",
        "# 텍스트 데이터 추출\n",
        "valid_texts = comment_valid['data'].tolist()\n",
        "print(f\"예측할 텍스트 수: {len(valid_texts)}\")\n",
        "\n",
        "# 토크나이징 (훈련 때와 동일한 설정 사용)\n",
        "valid_encodings = tokenizer(\n",
        "    valid_texts, \n",
        "    truncation=True, \n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "# TensorFlow 데이터셋으로 변환\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(dict(valid_encodings))\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# 예측 실행\n",
        "predictions = model.predict(valid_dataset)\n",
        "print(\"예측 완료!\")\n",
        "\n",
        "# %%\n",
        "predictions.logits\n",
        "\n",
        "# %%\n",
        "# 예측 결과 분석 및 평가\n",
        "print(\"예측 결과 분석 중...\")\n",
        "\n",
        "# logits에서 클래스 예측값으로 변환\n",
        "# logits : 모델이 출력하는 원시 점수 (확률화되기 전)\n",
        "# argmax : 가장 큰 값의 인덱스를 반환\n",
        "# axis=1 : 행 방향으로 최대값의 인덱스를 찾음\n",
        "# 큰 값이 예측 레이블이 됨\n",
        "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
        "\n",
        "# 예측 결과를 확률로 변환하기 위해 softmax 함수를 사용\n",
        "# softmax : 확률로 변환\n",
        "# axis=1 : 행 방향으로 최대값의 인덱스를 찾음\n",
        "# numpy() : numpy 배열로 변환\n",
        "prediction_probs = tf.nn.softmax(predictions.logits, axis=1).numpy()\n",
        "\n",
        "print(predicted_labels[:10])\n",
        "print(prediction_probs[:10])\n",
        "\n",
        "# 실제 레이블 변환 (부정=0, 긍정=1)  \n",
        "true_labels = comment_valid['label'].apply(lambda x: 0 if x == '부정' else 1).values\n",
        "\n",
        "# 성능 지표 계산\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f'정확도: {accuracy:.4f}')\n",
        "\n",
        "# 상세한 분류 보고서 출력\n",
        "print('\\n분류 보고서:')\n",
        "report = classification_report(true_labels, predicted_labels, target_names=['부정', '긍정'])\n",
        "print(report)\n",
        "\n",
        "# 혼동 행렬 시각화\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['부정', '긍정'], \n",
        "            yticklabels=['부정', '긍정'])\n",
        "plt.title('혼동 행렬 (Confusion Matrix)')\n",
        "plt.xlabel('예측 레이블')\n",
        "plt.ylabel('실제 레이블')\n",
        "plt.show()\n",
        "\n",
        "# 예측 결과를 DataFrame으로 저장\n",
        "results_df = pd.DataFrame({\n",
        "    '텍스트': comment_valid['data'],\n",
        "    '실제 레이블': comment_valid['label'],\n",
        "    '예측 레이블': ['부정' if label == 0 else '긍정' for label in predicted_labels],\n",
        "    '부정 확률': prediction_probs[:, 0],\n",
        "    '긍정 확률': prediction_probs[:, 1],\n",
        "    '예측 신뢰도': np.max(prediction_probs, axis=1)\n",
        "})\n",
        "\n",
        "print(\"\\n예측 결과 샘플:\")\n",
        "print(results_df[['텍스트', '실제 레이블', '예측 레이블', '예측 신뢰도']].head())\n",
        "\n",
        "# %%\n",
        "print(results_df.head())\n",
        "\n",
        "# %%\n",
        "# 잘못 예측된 샘플 분석\n",
        "incorrect_predictions = results_df[results_df['실제 레이블'] != results_df['예측 레이블']].copy()\n",
        "print(f\"\\n잘못 예측된 샘플 수: {len(incorrect_predictions)}\")\n",
        "\n",
        "if len(incorrect_predictions) > 0:\n",
        "    print(\"\\n잘못 예측된 샘플들:\")\n",
        "    print(incorrect_predictions[['텍스트', '실제 레이블', '예측 레이블', '예측 신뢰도']].head(10))\n",
        "    \n",
        "    # 신뢰도가 낮은 예측들 확인\n",
        "    low_confidence = results_df[results_df['예측 신뢰도'] < 0.8].copy()\n",
        "    print(f\"\\n신뢰도가 낮은 예측 수 (< 80%): {len(low_confidence)}\")\n",
        "    \n",
        "    if len(low_confidence) > 0:\n",
        "        print(\"신뢰도가 낮은 예측 샘플들:\")\n",
        "        print(low_confidence[['텍스트', '실제 레이블', '예측 레이블', '예측 신뢰도']].head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"모델 성능 요약:\")\n",
        "print(f\"- 총 테스트 샘플: {len(results_df)}\")\n",
        "print(f\"- 정확한 예측: {len(results_df) - len(incorrect_predictions)}\")\n",
        "print(f\"- 잘못된 예측: {len(incorrect_predictions)}\")\n",
        "print(f\"- 정확도: {accuracy:.4f}\")\n",
        "print(f\"- 평균 예측 신뢰도: {results_df['예측 신뢰도'].mean():.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "# %%\n",
        "# 모델 및 결과 저장\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# 저장 디렉토리 생성\n",
        "save_dir = \"model_output\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 현재 시간을 포함한 파일명 생성\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# 모델 저장 (SavedModel 형식과 H5 형식 모두 저장)\n",
        "model_path = os.path.join(save_dir, f\"sentiment_bert_model_{timestamp}\")\n",
        "h5_model_path = os.path.join(save_dir, f\"sentiment_bert_model_{timestamp}.h5\")\n",
        "\n",
        "try:\n",
        "    # SavedModel 형식 저장 (권장)\n",
        "    model.save(model_path)\n",
        "    print(f\"SavedModel 형식으로 모델이 저장되었습니다: {model_path}\")\n",
        "    \n",
        "    # H5 형식 저장 (가중치만 저장)\n",
        "    model.save_weights(h5_model_path)\n",
        "    print(f\"H5 형식으로 모델 가중치가 저장되었습니다: {h5_model_path}\")\n",
        "    \n",
        "    # 전체 모델을 H5 형식으로 저장 시도 (일부 모델에서는 제한이 있을 수 있음)\n",
        "    h5_full_model_path = os.path.join(save_dir, f\"sentiment_bert_full_model_{timestamp}.h5\")\n",
        "    try:\n",
        "        model.save(h5_full_model_path, save_format='h5')\n",
        "        print(f\"H5 형식으로 전체 모델이 저장되었습니다: {h5_full_model_path}\")\n",
        "    except Exception as h5_e:\n",
        "        print(f\"H5 형식 전체 모델 저장 중 오류 (가중치만 저장됨): {h5_e}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"모델 저장 중 오류 발생: {e}\")\n",
        "\n",
        "# 결과 CSV 저장\n",
        "results_path = os.path.join(save_dir, f\"prediction_results_{timestamp}.csv\")\n",
        "try:\n",
        "    results_df.to_csv(results_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"예측 결과가 저장되었습니다: {results_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"결과 저장 중 오류 발생: {e}\")\n",
        "\n",
        "# 모델 설정 정보 저장\n",
        "config_info = {\n",
        "    'model_name': BERT_MODEL_NAME,\n",
        "    'max_length': MAX_LENGTH,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'epochs': EPOCHS,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'test_size': TEST_SIZE,\n",
        "    'final_accuracy': accuracy,\n",
        "    'timestamp': timestamp\n",
        "}\n",
        "\n",
        "config_path = os.path.join(save_dir, f\"model_config_{timestamp}.txt\")\n",
        "try:\n",
        "    with open(config_path, 'w', encoding='utf-8') as f:\n",
        "        for key, value in config_info.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "    print(f\"모델 설정 정보가 저장되었습니다: {config_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"설정 저장 중 오류 발생: {e}\")\n",
        "\n",
        "print(f\"\\n모든 결과가 '{save_dir}' 디렉토리에 저장되었습니다.\")\n",
        "results_df = pd.DataFrame({\n",
        "    '텍스트': comment_test['data'],\n",
        "    '실제 레이블': comment_test['label'],\n",
        "    '예측 레이블': ['부정' if label == 0 else '긍정' for label in predicted_labels]\n",
        "})\n",
        "\n",
        "# 실제 레이블과 예측 레이블이 다른 경우만 필터링\n",
        "incorrect_predictions = results_df[results_df['실제 레이블'] != results_df['예측 레이블']]\n",
        "\n",
        "# 코드 실행 완료 시간 기록 및 총 소요 시간 계산\n",
        "end_time = time.time()\n",
        "end_datetime = datetime.now()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"\\n코드 실행 완료: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"총 실행 소요 시간: {total_duration/60:.2f}분 ({total_duration:.2f}초)\")\n",
        "\n",
        "incorrect_predictions\n",
        "\n",
        "# %%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
