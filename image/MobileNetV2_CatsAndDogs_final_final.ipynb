{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # ğŸ±ğŸ¶ MobileNetV2ë¥¼ í™œìš©í•œ ê³ ì–‘ì´/ê°œ ë¶„ë¥˜ í”„ë¡œì íŠ¸\n",
    "# \n",
    "# ## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "# - **ëª©í‘œ**: MobileNetV2 ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì„ í™œìš©í•œ Transfer Learningìœ¼ë¡œ ê³ ì–‘ì´/ê°œ ì´ë¯¸ì§€ ë¶„ë¥˜\n",
    "# - **ë°ì´í„°ì…‹**: Googleì˜ cats_and_dogs_filtered ë°ì´í„°ì…‹\n",
    "# - **ëª¨ë¸**: MobileNetV2 (ImageNet ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©)\n",
    "# - **ê¸°ë²•**: Transfer Learning & Fine-tuning\n",
    "\n",
    "# %% [markdown]\n",
    "# ## A. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "# %%\n",
    "# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (Colab í™˜ê²½ì—ì„œë§Œ ì‹¤í–‰)\n",
    "# ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Colab í™˜ê²½ í™•ì¸\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ğŸ” Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ğŸ” ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
    "\n",
    "# Colabì—ì„œë§Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰\n",
    "if IN_COLAB:\n",
    "    # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "    os.system('wget https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip')\n",
    "    os.system('unzip cats_and_dogs_filtered.zip')\n",
    "else:\n",
    "    print(\"ğŸ“¥ ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\")\n",
    "    print(\"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\")\n",
    "\n",
    "# %%\n",
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ”§ TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "\n",
    "# %%\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸ ë° ì„¤ì •\n",
    "if IN_COLAB:\n",
    "    # Colab í™˜ê²½ì—ì„œì˜ ê²½ë¡œ\n",
    "    base_path = '/content/cats_and_dogs_filtered'\n",
    "else:\n",
    "    # ë¡œì»¬ í™˜ê²½ì—ì„œì˜ ê²½ë¡œ (ì‚¬ìš©ìê°€ ë°ì´í„°ì…‹ì„ ì €ì¥í•œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
    "    base_path = './cats_and_dogs_filtered'  # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì••ì¶• í•´ì œí–ˆë‹¤ê³  ê°€ì •\n",
    "\n",
    "# ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "if os.path.exists(base_path):\n",
    "    print(f\"âœ… ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸: {base_path}\")\n",
    "    \n",
    "    # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "    train_dir = os.path.join(base_path, 'train')\n",
    "    validation_dir = os.path.join(base_path, 'validation')\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸\n",
    "    print(f\"\\nğŸ“ ë°ì´í„°ì…‹ êµ¬ì¡°:\")\n",
    "    print(f\"   í›ˆë ¨ ë°ì´í„°: {train_dir}\")\n",
    "    if os.path.exists(train_dir):\n",
    "        train_cats = len(os.listdir(os.path.join(train_dir, 'cats')))\n",
    "        train_dogs = len(os.listdir(os.path.join(train_dir, 'dogs')))\n",
    "        print(f\"     - ê³ ì–‘ì´: {train_cats}ì¥\")\n",
    "        print(f\"     - ê°œ: {train_dogs}ì¥\")\n",
    "    \n",
    "    print(f\"   ê²€ì¦ ë°ì´í„°: {validation_dir}\")\n",
    "    if os.path.exists(validation_dir):\n",
    "        val_cats = len(os.listdir(os.path.join(validation_dir, 'cats')))\n",
    "        val_dogs = len(os.listdir(os.path.join(validation_dir, 'dogs')))\n",
    "        print(f\"     - ê³ ì–‘ì´: {val_cats}ì¥\")\n",
    "        print(f\"     - ê°œ: {val_dogs}ì¥\")\n",
    "else:\n",
    "    print(f\"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_path}\")\n",
    "    print(\"ğŸ“¥ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì••ì¶•ì„ í•´ì œí•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## B. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë°ì´í„°ì…‹ ìƒì„±\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •ê°’ ì •ì˜\n",
    "\n",
    "# %%\n",
    "# ğŸ›ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
    "INPUT_SHAPE = (224, 224, 3)  # MobileNetV2 ì…ë ¥ í¬ê¸°\n",
    "NUM_CLASSES = 2              # ê³ ì–‘ì´(0), ê°œ(1)\n",
    "BATCH_SIZE = 32              # ë°°ì¹˜ í¬ê¸°\n",
    "IMAGE_SIZE = (224, 224)      # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í¬ê¸°\n",
    "LEARNING_RATE = 0.0003       # í•™ìŠµë¥ \n",
    "EPOCHS = 10                  # í•™ìŠµ ì—í­ ìˆ˜\n",
    "\n",
    "print(\"âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"   - ì…ë ¥ í¬ê¸°: {INPUT_SHAPE}\")\n",
    "print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {NUM_CLASSES}\")\n",
    "print(f\"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\"   - í•™ìŠµë¥ : {LEARNING_RATE}\")\n",
    "print(f\"   - ì—í­ ìˆ˜: {EPOCHS}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬\n",
    "\n",
    "# %%\n",
    "# ğŸ—‚ï¸ í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±\n",
    "# image_dataset_from_directory: í´ë” êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë¼ë²¨ë§\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    label_mode=\"binary\",        # ì´ì§„ ë¶„ë¥˜ (ê³ ì–‘ì´=0, ê°œ=1)\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,      # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "    seed=42,                    # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ\n",
    "    shuffle=True,               # ë°ì´í„° ì…”í”Œë§\n",
    ")\n",
    "\n",
    "# ğŸ—‚ï¸ ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=validation_dir,\n",
    "    label_mode=\"binary\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    seed=42,\n",
    "    shuffle=False,              # ê²€ì¦ ë°ì´í„°ëŠ” ì…”í”Œí•˜ì§€ ì•ŠìŒ\n",
    ")\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   - í›ˆë ¨ ë°ì´í„°: {len(train_ds)} ë°°ì¹˜\")\n",
    "print(f\"   - ê²€ì¦ ë°ì´í„°: {len(test_ds)} ë°°ì¹˜\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ì´ë¦„ í™•ì¸\n",
    "class_names = train_ds.class_names\n",
    "print(f\"   - í´ë˜ìŠ¤: {class_names}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. ë°ì´í„° ì‹œê°í™”\n",
    "\n",
    "# %%\n",
    "# ğŸ“Š ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "plt.figure(figsize=(15, 10))\n",
    "for images, labels in train_ds.take(1):  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "    for i in range(min(20, len(images))):  # ìµœëŒ€ 20ê°œ ì´ë¯¸ì§€ í‘œì‹œ\n",
    "        ax = plt.subplot(4, 5, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        # ë¼ë²¨ì„ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜\n",
    "        label_idx = int(labels[i].numpy())\n",
    "        plt.title(f'{class_names[label_idx]}', fontsize=12)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle('ğŸ–¼ï¸ í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## C. MobileNetV2 Transfer Learning ëª¨ë¸ êµ¬ì¶•\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. ì‚¬ì „í›ˆë ¨ëœ MobileNetV2 ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "# %%\n",
    "# ğŸ§  MobileNetV2 ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ë¡œë“œ\n",
    "# ImageNetìœ¼ë¡œ ì‚¬ì „í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©, ë¶„ë¥˜ì¸µ(top layer) ì œì™¸\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    weights='imagenet',    # ImageNet ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜\n",
    "    include_top=False      # ìµœìƒìœ„ ë¶„ë¥˜ì¸µ ì œì™¸ (ìš°ë¦¬ê°€ ì§ì ‘ ì¶”ê°€í•  ì˜ˆì •)\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ MobileNetV2 ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"   - ì´ ë ˆì´ì–´ ìˆ˜: {len(base_model.layers)}\")\n",
    "print(f\"   - ì¶œë ¥ í˜•íƒœ: {base_model.output_shape}\")\n",
    "\n",
    "# ğŸ”’ ë² ì´ìŠ¤ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³ ì • (Transfer Learning)\n",
    "# ì‚¬ì „í›ˆë ¨ëœ íŠ¹ì„± ì¶”ì¶œê¸°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ìƒˆë¡œìš´ ë¶„ë¥˜ì¸µë§Œ í•™ìŠµ\n",
    "base_model.trainable = False\n",
    "print(\"ğŸ”’ ë² ì´ìŠ¤ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³ ì • ì™„ë£Œ!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. ì „ì²´ ëª¨ë¸ êµ¬ì¶•\n",
    "\n",
    "# %%\n",
    "# ğŸ—ï¸ ì „ì²´ ëª¨ë¸ êµ¬ì¶•\n",
    "# Sequential API ëŒ€ì‹  Functional API ì‚¬ìš©ìœ¼ë¡œ ë” ìœ ì—°í•œ ëª¨ë¸ êµ¬ì„±\n",
    "\n",
    "inputs = tf.keras.Input(shape=INPUT_SHAPE)\n",
    "\n",
    "# 1ï¸âƒ£ ì „ì²˜ë¦¬ ë ˆì´ì–´: í”½ì…€ ê°’ì„ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n",
    "# MobileNetV2ëŠ” [-1, 1] ë²”ìœ„ì˜ ì…ë ¥ì„ ê¸°ëŒ€í•¨\n",
    "x = tf.keras.layers.Rescaling(1./127.5, offset=-1)(inputs)\n",
    "\n",
    "# 2ï¸âƒ£ ì‚¬ì „í›ˆë ¨ëœ MobileNetV2 íŠ¹ì„± ì¶”ì¶œ\n",
    "# training=False: ë°°ì¹˜ ì •ê·œí™” ë ˆì´ì–´ë¥¼ ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# 3ï¸âƒ£ ì „ì—­ í‰ê·  í’€ë§: (7, 7, 1280) â†’ (1280,)\n",
    "# ê° íŠ¹ì„± ë§µì˜ í‰ê· ê°’ì„ ê³„ì‚°í•˜ì—¬ ì°¨ì› ì¶•ì†Œ\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# 4ï¸âƒ£ ë“œë¡­ì•„ì›ƒ: ê³¼ì í•© ë°©ì§€\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# 5ï¸âƒ£ ì¶œë ¥ì¸µ: ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™”\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# 6ï¸âƒ£ ëª¨ë¸ ìƒì„±\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(\"ğŸ—ï¸ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "model.summary()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. ëª¨ë¸ ì»´íŒŒì¼\n",
    "\n",
    "# %%\n",
    "# âš™ï¸ ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),  # Adam ì˜µí‹°ë§ˆì´ì €\n",
    "    loss='binary_crossentropy',      # ì´ì§„ ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜\n",
    "    metrics=['accuracy']             # í‰ê°€ ì§€í‘œ\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ!\")\n",
    "print(f\"   - ì˜µí‹°ë§ˆì´ì €: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"   - ì†ì‹¤ í•¨ìˆ˜: Binary Crossentropy\")\n",
    "print(f\"   - í‰ê°€ ì§€í‘œ: Accuracy\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## D. ì½œë°± í•¨ìˆ˜ ì„¤ì •\n",
    "\n",
    "# %%\n",
    "# ğŸ“ ì½œë°± í•¨ìˆ˜ ì„¤ì •\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# 1ï¸âƒ£ ì¡°ê¸° ì¢…ë£Œ: ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # ëª¨ë‹ˆí„°ë§í•  ì§€í‘œ\n",
    "    mode='min',            # ìµœì†Ÿê°’ì„ ì¶”ì \n",
    "    verbose=1,             # ë¡œê·¸ ì¶œë ¥\n",
    "    patience=5,            # 5 ì—í­ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨\n",
    "    restore_best_weights=True  # ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ë¡œ ë³µì›\n",
    ")\n",
    "\n",
    "# 2ï¸âƒ£ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "checkpoint_path = \"best_mobilenetv2_cats_dogs.keras\"\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,   # ìµœê³  ì„±ëŠ¥ì¼ ë•Œë§Œ ì €ì¥\n",
    "    monitor='val_accuracy', # ê²€ì¦ ì •í™•ë„ ê¸°ì¤€\n",
    "    mode='max',            # ìµœëŒ“ê°’ì„ ì¶”ì \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3ï¸âƒ£ í•™ìŠµë¥  ê°ì†Œ: ì„±ëŠ¥ ê°œì„ ì´ ë©ˆì¶”ë©´ í•™ìŠµë¥  ê°ì†Œ\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,            # í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ\n",
    "    patience=3,            # 3 ì—í­ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ê°ì†Œ\n",
    "    min_lr=1e-7,          # ìµœì†Œ í•™ìŠµë¥ \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]\n",
    "print(\"ğŸ“ ì½œë°± í•¨ìˆ˜ ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## E. ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "# %%\n",
    "# ğŸ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
    "print(\"ğŸ“ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "print(f\"   - ì—í­: {EPOCHS}\")\n",
    "print(f\"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,                    # í›ˆë ¨ ë°ì´í„°\n",
    "    validation_data=test_ds,     # ê²€ì¦ ë°ì´í„°\n",
    "    epochs=EPOCHS,               # í•™ìŠµ ì—í­ ìˆ˜\n",
    "    callbacks=callbacks,         # ì½œë°± í•¨ìˆ˜ë“¤\n",
    "    verbose=1                    # í•™ìŠµ ê³¼ì • ì¶œë ¥\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### ëª¨ë¸ ì €ì¥\n",
    "\n",
    "# %%\n",
    "# ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ë° ì €ì¥\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)\n",
    "    print(f\"âœ… ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë¡œë“œ: {checkpoint_path}\")\n",
    "\n",
    "# ì „ì²´ ëª¨ë¸ ì €ì¥ (ì—¬ëŸ¬ í˜•ì‹ìœ¼ë¡œ)\n",
    "model.save('cats_dogs_mobilenetv2_complete_model.keras')  # Keras ë„¤ì´í‹°ë¸Œ í˜•ì‹ (ê¶Œì¥)\n",
    "model.save('cats_dogs_mobilenetv2_complete_model.h5')     # HDF5 í˜•ì‹\n",
    "# SavedModel í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (TensorFlow Serving ë“±ì—ì„œ ì‚¬ìš©)\n",
    "model.export('cats_dogs_mobilenetv2_savedmodel')\n",
    "\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(\"   - Keras ë„¤ì´í‹°ë¸Œ í˜•ì‹: cats_dogs_mobilenetv2_complete_model.keras\")\n",
    "print(\"   - HDF5 í˜•ì‹: cats_dogs_mobilenetv2_complete_model.h5\")\n",
    "print(\"   - SavedModel í˜•ì‹: cats_dogs_mobilenetv2_savedmodel\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## F. í•™ìŠµ ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "\n",
    "# %%\n",
    "# ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "def plot_training_history(history):\n",
    "    \"\"\"í•™ìŠµ íˆìŠ¤í† ë¦¬ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # ì •í™•ë„ ê³¡ì„ \n",
    "    ax1.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„', marker='s')\n",
    "    ax1.set_title('ğŸ“Š ëª¨ë¸ ì •í™•ë„', fontsize=14)\n",
    "    ax1.set_xlabel('ì—í­')\n",
    "    ax1.set_ylabel('ì •í™•ë„')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ì†ì‹¤ ê³¡ì„ \n",
    "    ax2.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤', marker='s')\n",
    "    ax2.set_title('ğŸ“‰ ëª¨ë¸ ì†ì‹¤', fontsize=14)\n",
    "    ax2.set_xlabel('ì—í­')\n",
    "    ax2.set_ylabel('ì†ì‹¤')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ì¶œë ¥\n",
    "    best_train_acc = max(history.history['accuracy'])\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    print(f\"ğŸ† ìµœê³  í›ˆë ¨ ì •í™•ë„: {best_train_acc:.4f}\")\n",
    "    print(f\"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}\")\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°\n",
    "plot_training_history(history)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. ëª¨ë¸ í‰ê°€\n",
    "\n",
    "# %%\n",
    "# ğŸ“Š ëª¨ë¸ í‰ê°€\n",
    "print(\"ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ í‰ê°€\n",
    "test_loss, test_accuracy = model.evaluate(test_ds, verbose=0)\n",
    "print(f\"âœ… ìµœì¢… ê²€ì¦ ì •í™•ë„: {test_accuracy:.4f}\")\n",
    "print(f\"âœ… ìµœì¢… ê²€ì¦ ì†ì‹¤: {test_loss:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## G. ì˜ˆì¸¡ ë° ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# %%\n",
    "# ğŸ”® ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ”® ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ì—ì„œ í•œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°\n",
    "batch_images, batch_labels = next(iter(test_ds))\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {batch_images.shape[0]}\")\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "predictions = model.predict(batch_images, verbose=0)\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i in range(min(16, len(batch_images))):  # ìµœëŒ€ 16ê°œ ì´ë¯¸ì§€ í‘œì‹œ\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    \n",
    "    # ì´ë¯¸ì§€ í‘œì‹œ (ì •ê·œí™”ëœ ì´ë¯¸ì§€ë¥¼ ì›ë˜ ë²”ìœ„ë¡œ ë³µì›)\n",
    "    img = batch_images[i].numpy()\n",
    "    img = (img + 1) / 2  # [-1, 1] â†’ [0, 1]\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë¼ë²¨ ë¹„êµ\n",
    "    true_label = int(batch_labels[i].numpy())\n",
    "    pred_label = predicted_classes[i]\n",
    "    confidence = predictions[i][0]\n",
    "    \n",
    "    # ì œëª© ì„¤ì • (ë§ìœ¼ë©´ ì´ˆë¡ìƒ‰, í‹€ë¦¬ë©´ ë¹¨ê°„ìƒ‰)\n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    title = f'ì‹¤ì œ: {class_names[true_label]}\\nì˜ˆì¸¡: {class_names[pred_label]}\\nì‹ ë¢°ë„: {confidence:.3f}'\n",
    "    plt.title(title, color=color, fontsize=10)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('ğŸ”® ì˜ˆì¸¡ ê²°ê³¼ (ì´ˆë¡ìƒ‰: ì •ë‹µ, ë¹¨ê°„ìƒ‰: ì˜¤ë‹µ)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚°\n",
    "correct_predictions = np.sum(predicted_classes == batch_labels.numpy())\n",
    "batch_accuracy = correct_predictions / len(batch_labels)\n",
    "print(f\"ğŸ“Š ë°°ì¹˜ ì •í™•ë„: {batch_accuracy:.4f} ({correct_predictions}/{len(batch_labels)})\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. ê°œë³„ ì´ë¯¸ì§€ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "\n",
    "# %%\n",
    "def predict_single_image(model, image_path, class_names):\n",
    "    \"\"\"\n",
    "    ê°œë³„ ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        model: í›ˆë ¨ëœ ëª¨ë¸\n",
    "        image_path: ì˜ˆì¸¡í•  ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "        predicted_class: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤\n",
    "        confidence: ì˜ˆì¸¡ ì‹ ë¢°ë„\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "        img = tf.keras.utils.load_img(image_path, target_size=IMAGE_SIZE)\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "        \n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        prediction = model.predict(img_array, verbose=0)\n",
    "        confidence = prediction[0][0]\n",
    "        predicted_class = class_names[int(confidence > 0.5)]\n",
    "        \n",
    "        return predicted_class, confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì´ë¯¸ì§€ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ì˜ˆì¸¡ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (ê²€ì¦ ë°ì´í„°ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¡œ)\n",
    "if os.path.exists(validation_dir):\n",
    "    # ê³ ì–‘ì´ ì´ë¯¸ì§€ í•˜ë‚˜ ì„ íƒ\n",
    "    cat_images = glob(os.path.join(validation_dir, 'cats', '*.jpg'))\n",
    "    if cat_images:\n",
    "        test_image_path = cat_images[0]\n",
    "        predicted_class, confidence = predict_single_image(model, test_image_path, class_names)\n",
    "        print(f\"ğŸ”® ê°œë³„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:\")\n",
    "        print(f\"   ì´ë¯¸ì§€: {os.path.basename(test_image_path)}\")\n",
    "        print(f\"   ì˜ˆì¸¡: {predicted_class}\")\n",
    "        print(f\"   ì‹ ë¢°ë„: {confidence:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ ì˜ˆì¸¡\n",
    "\n",
    "# %%\n",
    "def predict_images_in_directory(model, directory_path, class_names, batch_size=32):\n",
    "    \"\"\"\n",
    "    ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ ë°°ì¹˜ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        model: í›ˆë ¨ëœ ëª¨ë¸\n",
    "        directory_path: ì˜ˆì¸¡í•  ì´ë¯¸ì§€ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
    "    \n",
    "    Returns:\n",
    "        results: [(íŒŒì¼ëª…, ì˜ˆì¸¡_í´ë˜ìŠ¤, ì‹ ë¢°ë„)] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“ ë””ë ‰í† ë¦¬ ì˜ˆì¸¡ ì‹œì‘: {directory_path}\")\n",
    "    \n",
    "    # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    \n",
    "    # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘\n",
    "    image_files = []\n",
    "    image_arrays = []\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(image_extensions):\n",
    "            img_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "                img = tf.keras.utils.load_img(img_path, target_size=IMAGE_SIZE)\n",
    "                img_array = tf.keras.utils.img_to_array(img)\n",
    "                \n",
    "                image_files.append(filename)\n",
    "                image_arrays.append(img_array)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not image_arrays:\n",
    "        print(\"âŒ ì˜ˆì¸¡í•  ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "    \n",
    "    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "    np_image_arrays = np.array(image_arrays)\n",
    "    print(f\"ğŸ“Š ì´ {len(np_image_arrays)}ê°œ ì´ë¯¸ì§€ ë°œê²¬\")\n",
    "    \n",
    "    # ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    predictions = model.predict(np_image_arrays, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    # ê²°ê³¼ ìƒì„±\n",
    "    results = []\n",
    "    for filename, pred in zip(image_files, predictions):\n",
    "        confidence = pred[0]\n",
    "        predicted_class = class_names[int(confidence > 0.5)]\n",
    "        results.append((filename, predicted_class, confidence))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ê³ ì–‘ì´ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "if os.path.exists(validation_dir):\n",
    "    cat_test_dir = os.path.join(validation_dir, 'cats')\n",
    "    if os.path.exists(cat_test_dir):\n",
    "        print(\"ğŸ± ê³ ì–‘ì´ ì´ë¯¸ì§€ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:\")\n",
    "        cat_results = predict_images_in_directory(model, cat_test_dir, class_names)\n",
    "        \n",
    "        # ê²°ê³¼ ë¶„ì„\n",
    "        correct_predictions = sum(1 for _, pred_class, _ in cat_results if pred_class == 'cats')\n",
    "        total_predictions = len(cat_results)\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        print(f\"ğŸ“Š ê³ ì–‘ì´ ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "        print(f\"   - ì´ ì´ë¯¸ì§€: {total_predictions}ê°œ\")\n",
    "        print(f\"   - ì •í™•í•œ ì˜ˆì¸¡: {correct_predictions}ê°œ\")\n",
    "        print(f\"   - ì •í™•ë„: {accuracy:.4f}\")\n",
    "        \n",
    "        # ì²˜ìŒ 5ê°œ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"   - ìƒ˜í”Œ ê²°ê³¼:\")\n",
    "        for i, (filename, pred_class, confidence) in enumerate(cat_results[:5]):\n",
    "            print(f\"     {i+1}. {filename}: {pred_class} (ì‹ ë¢°ë„: {confidence:.3f})\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "\n",
    "# %%\n",
    "def save_predictions_to_csv(results, output_filename):\n",
    "    \"\"\"\n",
    "    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        results: [(íŒŒì¼ëª…, ì˜ˆì¸¡_í´ë˜ìŠ¤, ì‹ ë¢°ë„)] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        output_filename: ì¶œë ¥ íŒŒì¼ëª…\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # DataFrame ìƒì„±\n",
    "    df = pd.DataFrame(results, columns=['filename', 'predicted_class', 'confidence'])\n",
    "    \n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"ğŸ’¾ ê²°ê³¼ê°€ {output_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ“Š ì €ì¥ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # ì˜ˆì¸¡ í´ë˜ìŠ¤ë³„ ê°œìˆ˜ í™•ì¸\n",
    "    print(f\"\\nğŸ“ˆ ì˜ˆì¸¡ í´ë˜ìŠ¤ë³„ ê°œìˆ˜:\")\n",
    "    print(df['predicted_class'].value_counts())\n",
    "    \n",
    "    # ì‹ ë¢°ë„ í†µê³„\n",
    "    print(f\"\\nğŸ“Š ì‹ ë¢°ë„ í†µê³„:\")\n",
    "    print(f\"   - í‰ê· : {df['confidence'].mean():.4f}\")\n",
    "    print(f\"   - ìµœëŒ€: {df['confidence'].max():.4f}\")\n",
    "    print(f\"   - ìµœì†Œ: {df['confidence'].min():.4f}\")\n",
    "\n",
    "# ê³ ì–‘ì´ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (ìœ„ì—ì„œ ì‹¤í–‰í–ˆë‹¤ë©´)\n",
    "if 'cat_results' in locals() and cat_results:\n",
    "    save_predictions_to_csv(cat_results, 'cats_predictions.csv')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## H. í”„ë¡œì íŠ¸ ìš”ì•½ ë° í•™ìŠµ ë‚´ìš©\n",
    "\n",
    "# %%\n",
    "print(\"ğŸ‰ í”„ë¡œì íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“š í•™ìŠµí•œ ì£¼ìš” ë‚´ìš©:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"1ï¸âƒ£ Transfer Learning ê°œë…ê³¼ í™œìš©\")\n",
    "print(\"   - ì‚¬ì „í›ˆë ¨ëœ MobileNetV2 ëª¨ë¸ í™œìš©\")\n",
    "print(\"   - ImageNet ê°€ì¤‘ì¹˜ë¥¼ ê³ ì–‘ì´/ê°œ ë¶„ë¥˜ì— ì „ì´\")\n",
    "print(\"   - ê³„ì‚° ë¹„ìš© ì ˆì•½ê³¼ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ê¸°ë²•\")\n",
    "print(\"   - image_dataset_from_directoryë¥¼ í™œìš©í•œ ìë™ ë¼ë²¨ë§\")\n",
    "print(\"   - ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•ê³¼ ì •ê·œí™”\")\n",
    "print(\"   - ë°°ì¹˜ ì²˜ë¦¬ì™€ ë°ì´í„° ì…”í”Œë§\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ëª¨ë¸ êµ¬ì¶• ê¸°ë²•\")\n",
    "print(\"   - Functional APIë¥¼ í™œìš©í•œ ìœ ì—°í•œ ëª¨ë¸ êµ¬ì„±\")\n",
    "print(\"   - ì „ì²˜ë¦¬ ë ˆì´ì–´ í†µí•©\")\n",
    "print(\"   - GlobalAveragePoolingê³¼ Dropout í™œìš©\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ í•™ìŠµ ìµœì í™” ê¸°ë²•\")\n",
    "print(\"   - EarlyStoppingìœ¼ë¡œ ê³¼ì í•© ë°©ì§€\")\n",
    "print(\"   - ModelCheckpointë¡œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\")\n",
    "print(\"   - ReduceLROnPlateauë¡œ í•™ìŠµë¥  ì¡°ì •\")\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\")\n",
    "print(\"   - í•™ìŠµ ê³¡ì„  ë¶„ì„\")\n",
    "print(\"   - ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\")\n",
    "print(\"   - ë°°ì¹˜ ì˜ˆì¸¡ê³¼ ê°œë³„ ì˜ˆì¸¡ êµ¬í˜„\")\n",
    "\n",
    "print(\"\\nğŸ¯ ì‹¤ë¬´ í™œìš© í¬ì¸íŠ¸:\")\n",
    "print(\"   - ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±\")\n",
    "print(\"   - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ ê°€ëŠ¥\")\n",
    "print(\"   - ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì— ì‘ìš© ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\nâœ… ë‹¤ìŒ ë‹¨ê³„ í•™ìŠµ ê¶Œì¥ì‚¬í•­:\")\n",
    "print(\"   - Fine-tuning: ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì˜ ì¼ë¶€ ë ˆì´ì–´ í•™ìŠµ\")\n",
    "print(\"   - Data Augmentation: ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©\")\n",
    "print(\"   - ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œ ë„ì „\")\n",
    "print(\"   - ë‹¤ë¥¸ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ (ResNet, EfficientNet ë“±) ë¹„êµ\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ì„±ê³µì ìœ¼ë¡œ Transfer Learningì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## I. í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì˜ˆì¸¡ ë° ë¶„ì„\n",
    "\n",
    "#!cp cats_and_dogs_filtered/validation/cats/* cats_and_dogs_filtered/test/\n",
    "#!cp cats_and_dogs_filtered/validation/dogs/* cats_and_dogs_filtered/test/\n",
    "\n",
    "# %%\n",
    "# í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
    "test_path = os.path.join('cats_and_dogs_filtered', 'test')\n",
    "\n",
    "# ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„\n",
    "if not os.path.exists(test_path):\n",
    "    test_path = './image/cats_and_dogs_filtered/test'\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "if os.path.exists(test_path):\n",
    "    print(f\"ğŸ” í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì˜ˆì¸¡ ì‹œì‘: {test_path}\")\n",
    "    \n",
    "    # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
    "    \n",
    "    # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘\n",
    "    image_files = []\n",
    "    image_arrays = []\n",
    "    actual_classes = []\n",
    "    \n",
    "    for filename in os.listdir(test_path):\n",
    "        if filename.lower().endswith(image_extensions):\n",
    "            img_path = os.path.join(test_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "                img = tf.keras.utils.load_img(img_path, target_size=IMAGE_SIZE)\n",
    "                img_array = tf.keras.utils.img_to_array(img)\n",
    "                \n",
    "                image_files.append(filename)\n",
    "                image_arrays.append(img_array)\n",
    "                \n",
    "                # íŒŒì¼ëª…ì—ì„œ ì‹¤ì œ í´ë˜ìŠ¤ ì¶”ì¶œ (cat.xxxx.jpg -> cats, dog.xxxx.jpg -> dogs)\n",
    "                if filename.lower().startswith('cat'):\n",
    "                    actual_classes.append('cats')\n",
    "                elif filename.lower().startswith('dog'):\n",
    "                    actual_classes.append('dogs')\n",
    "                else:\n",
    "                    actual_classes.append('unknown')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}\")\n",
    "                continue\n",
    "    \n",
    "    if image_arrays:\n",
    "        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "        np_image_arrays = np.array(image_arrays)\n",
    "        print(f\"ğŸ“Š ì´ {len(np_image_arrays)}ê°œ ì´ë¯¸ì§€ ë°œê²¬\")\n",
    "        \n",
    "        # ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        print(\"ğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\")\n",
    "        predictions = model.predict(np_image_arrays, batch_size=32, verbose=1)\n",
    "        \n",
    "        # ê²°ê³¼ ìƒì„±\n",
    "        test_results = []\n",
    "        for filename, pred, actual in zip(image_files, predictions, actual_classes):\n",
    "            confidence = pred[0]\n",
    "            predicted_class = class_names[int(confidence > 0.5)]\n",
    "            test_results.append((filename, predicted_class, confidence, actual))\n",
    "        \n",
    "        # ê²°ê³¼ ë¶„ì„\n",
    "        total_images = len(test_results)\n",
    "        correct_predictions = sum(1 for _, pred, _, actual in test_results \n",
    "                                if pred == actual and actual != 'unknown')\n",
    "        \n",
    "        # í´ë˜ìŠ¤ë³„ ë¶„ì„\n",
    "        cats_images = [r for r in test_results if r[3] == 'cats']\n",
    "        dogs_images = [r for r in test_results if r[3] == 'dogs']\n",
    "        \n",
    "        cats_correct = sum(1 for _, pred, _, actual in cats_images if pred == actual)\n",
    "        dogs_correct = sum(1 for _, pred, _, actual in dogs_images if pred == actual)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ğŸ“ ì´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {total_images}ê°œ\")\n",
    "        print(f\"ğŸ± ê³ ì–‘ì´ ì´ë¯¸ì§€: {len(cats_images)}ê°œ\")\n",
    "        print(f\"ğŸ¶ ê°œ ì´ë¯¸ì§€: {len(dogs_images)}ê°œ\")\n",
    "        \n",
    "        if len(cats_images) > 0:\n",
    "            cats_accuracy = cats_correct / len(cats_images)\n",
    "            print(f\"ğŸ± ê³ ì–‘ì´ ì •í™•ë„: {cats_accuracy:.4f} ({cats_correct}/{len(cats_images)})\")\n",
    "        \n",
    "        if len(dogs_images) > 0:\n",
    "            dogs_accuracy = dogs_correct / len(dogs_images)\n",
    "            print(f\"ğŸ¶ ê°œ ì •í™•ë„: {dogs_accuracy:.4f} ({dogs_correct}/{len(dogs_images)})\")\n",
    "        \n",
    "        if len(cats_images) + len(dogs_images) > 0:\n",
    "            overall_accuracy = correct_predictions / (len(cats_images) + len(dogs_images))\n",
    "            print(f\"ğŸ¯ ì „ì²´ ì •í™•ë„: {overall_accuracy:.4f} ({correct_predictions}/{len(cats_images) + len(dogs_images)})\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ í´ë˜ìŠ¤ë³„ ê°œìˆ˜\n",
    "        pred_counts = {}\n",
    "        for _, pred, _, _ in test_results:\n",
    "            pred_counts[pred] = pred_counts.get(pred, 0) + 1\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\")\n",
    "        for class_name, count in pred_counts.items():\n",
    "            print(f\"   - {class_name}: {count}ê°œ\")\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ë¶„ì„\n",
    "        confidences = [conf for _, _, conf, _ in test_results]\n",
    "        print(f\"\\nğŸ“Š ì˜ˆì¸¡ ì‹ ë¢°ë„ í†µê³„:\")\n",
    "        print(f\"   - í‰ê·  ì‹ ë¢°ë„: {np.mean(confidences):.4f}\")\n",
    "        print(f\"   - ìµœê³  ì‹ ë¢°ë„: {np.max(confidences):.4f}\")\n",
    "        print(f\"   - ìµœì € ì‹ ë¢°ë„: {np.min(confidences):.4f}\")\n",
    "        print(f\"   - í‘œì¤€í¸ì°¨: {np.std(confidences):.4f}\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\nğŸ” ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\")\n",
    "        print(f\"{'íŒŒì¼ëª…':<20} {'ì‹¤ì œ':<8} {'ì˜ˆì¸¡':<8} {'ì‹ ë¢°ë„':<8} {'ê²°ê³¼'}\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, (filename, pred, conf, actual) in enumerate(test_results[:10]):\n",
    "            result_mark = \"âœ…\" if pred == actual else \"âŒ\"\n",
    "            print(f\"{filename:<20} {actual:<8} {pred:<8} {conf:<8.3f} {result_mark}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"âŒ ì˜ˆì¸¡í•  ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_path}\")\n",
    "\n",
    "# %%\n",
    "# í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ìƒì„¸ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "if 'test_results' in locals() and test_results:\n",
    "    # DataFrame ìƒì„±\n",
    "    df = pd.DataFrame(test_results, columns=['filename', 'predicted_class', 'confidence', 'actual_class'])\n",
    "    \n",
    "    # ì •ë‹µ ì—¬ë¶€ ì¶”ê°€\n",
    "    df['is_correct'] = df.apply(lambda row: row['predicted_class'] == row['actual_class'] \n",
    "                               if row['actual_class'] != 'unknown' else None, axis=1)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ (ê³ ì‹ ë¢°ë„/ì €ì‹ ë¢°ë„)\n",
    "    df['confidence_level'] = df['confidence'].apply(lambda x: 'high' if x > 0.8 else 'medium' if x > 0.6 else 'low')\n",
    "    \n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    output_filename = 'test_predictions_detailed.csv'\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ {output_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ“Š ì €ì¥ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # í†µê³„ ìš”ì•½\n",
    "    if df['is_correct'].notna().any():\n",
    "        accuracy = df['is_correct'].mean()\n",
    "        print(f\"\\nğŸ“ˆ ì €ì¥ëœ ë°ì´í„° í†µê³„:\")\n",
    "        print(f\"   - ì •í™•ë„: {accuracy:.4f}\")\n",
    "        print(f\"   - ì •ë‹µ: {df['is_correct'].sum()}ê°œ\")\n",
    "        print(f\"   - ì˜¤ë‹µ: {(~df['is_correct']).sum()}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì‹ ë¢°ë„ ë ˆë²¨ ë¶„í¬:\")\n",
    "    print(df['confidence_level'].value_counts())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### í‹€ë¦° ì˜ˆì¸¡ ë¶„ì„\n",
    "\n",
    "# %%\n",
    "# í‹€ë¦° ì˜ˆì¸¡ë“¤ ìƒì„¸ ë¶„ì„\n",
    "if 'test_results' in locals() and test_results:\n",
    "    # í‹€ë¦° ì˜ˆì¸¡ë“¤ë§Œ í•„í„°ë§\n",
    "    incorrect_predictions = [(filename, pred, conf, actual) \n",
    "                           for filename, pred, conf, actual in test_results \n",
    "                           if pred != actual and actual != 'unknown']\n",
    "    \n",
    "    if incorrect_predictions:\n",
    "        print(\"\\nâŒ í‹€ë¦° ì˜ˆì¸¡ ë¶„ì„\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"ì´ {len(incorrect_predictions)}ê°œì˜ í‹€ë¦° ì˜ˆì¸¡\")\n",
    "        \n",
    "        # ì‹ ë¢°ë„ë³„ í‹€ë¦° ì˜ˆì¸¡ ë¶„ì„\n",
    "        high_conf_wrong = [r for r in incorrect_predictions if r[2] > 0.8]\n",
    "        medium_conf_wrong = [r for r in incorrect_predictions if 0.6 <= r[2] <= 0.8]\n",
    "        low_conf_wrong = [r for r in incorrect_predictions if r[2] < 0.6]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì‹ ë¢°ë„ë³„ í‹€ë¦° ì˜ˆì¸¡:\")\n",
    "        print(f\"   - ê³ ì‹ ë¢°ë„(>0.8): {len(high_conf_wrong)}ê°œ\")\n",
    "        print(f\"   - ì¤‘ì‹ ë¢°ë„(0.6-0.8): {len(medium_conf_wrong)}ê°œ\")\n",
    "        print(f\"   - ì €ì‹ ë¢°ë„(<0.6): {len(low_conf_wrong)}ê°œ\")\n",
    "        \n",
    "        # ê°€ì¥ í™•ì‹ ìˆê²Œ í‹€ë¦° ì˜ˆì¸¡ë“¤\n",
    "        if high_conf_wrong:\n",
    "            print(f\"\\nğŸš¨ ê°€ì¥ í™•ì‹ ìˆê²Œ í‹€ë¦° ì˜ˆì¸¡ë“¤:\")\n",
    "            sorted_wrong = sorted(high_conf_wrong, key=lambda x: x[2], reverse=True)\n",
    "            for filename, pred, conf, actual in sorted_wrong[:5]:\n",
    "                print(f\"   - {filename}: {actual} â†’ {pred} (ì‹ ë¢°ë„: {conf:.3f})\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "    else:\n",
    "        print(\"\\nğŸ‰ ëª¨ë“  ì˜ˆì¸¡ì´ ì •í™•í•©ë‹ˆë‹¤!\")\n",
    "\n",
    "# %%\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
